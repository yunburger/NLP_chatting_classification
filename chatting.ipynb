{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PX3iTW8SFx2o"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dM3pJ5SDdfp"
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3zPBY-zDfVn"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VkpbbuqmDhqe"
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, emded_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=emded_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=emded_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bc515bLqDv32"
   },
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['20,000원', '을', '받으세요', '.'], ['20,000원', '받기', '완료', '!', '받은', '카카오', '페이', '머니는', '송금', '및', '온', '/', '오프라인', '결제', '는', '물론', ',', '투자', '도', '가능해요', '.'], ['이모티콘'], ['따룽행행']]\n",
      "[0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "_x = []\n",
    "_y = []\n",
    "stopwords = ['\\n']\n",
    "f = open(\"x_train.txt\", 'r', encoding = 'UTF-8')\n",
    "yf = open(\"y_train.txt\", 'r', encoding = 'UTF-8')\n",
    "while True :\n",
    "    line = f.readline()\n",
    "    if not line : break\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(line) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    _x.append(temp_X)\n",
    "    temp_Y = yf.readline()[0]\n",
    "    _y.append(int(temp_Y))\n",
    "    \n",
    "    \n",
    "print(_x[:4])\n",
    "print(_y[:4])\n",
    "\n",
    "f.close()\n",
    "yf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qeqkss4dDkJO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4011 Training sequences\n",
      "1003 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 6000  # Only consider the top 20k words\n",
    "maxlen = 12  # Only consider the first 200 words of each movie review\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(_x, _y, test_size=0.2, random_state=321)\n",
    "\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_val = tokenizer.texts_to_sequences(x_val)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1614, 43, 4869, 23], [456, 15], [881, 882, 1279]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이 : 46\n",
      "[[   0    0    0    0    0    0    0    0 1614   43 4869   23]\n",
      " [   0    0    0    0    0    0    0    0    0    0  456   15]\n",
      " [   0    0    0    0    0    0    0    0    0  881  882 1279]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print('최대 길이 :',max(len(l) for l in x_train))\n",
    "x_train = pad_sequences(x_train, maxlen = maxlen)\n",
    "x_val = pad_sequences(x_val, maxlen = maxlen)\n",
    "\n",
    "print(x_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCOLyyUFDmau"
   },
   "source": [
    "# Create classifier model using teansformer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6fFpyNzDlLO"
   },
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 12 \n",
    "vocab_size = 6000 \n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 12)]              0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 12, 32)            192384    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 12, 32)            6464      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 199,550\n",
      "Trainable params: 199,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\d518_1\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'errno'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\d518_1\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format)\u001b[0m\n\u001b[0;32m   1872\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1873\u001b[1;33m             p = subprocess.Popen(\n\u001b[0m\u001b[0;32m   1874\u001b[0m                 \u001b[0mcmdline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d518_1\\appdata\\local\\programs\\python\\python38\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 854\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d518_1\\appdata\\local\\programs\\python\\python38\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1307\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1308\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-102852267fc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m tf.keras.utils.plot_model(\n\u001b[0m\u001b[0;32m      3\u001b[0m     model, to_file='model.png', show_shapes=False, show_layer_names=True)\n",
      "\u001b[1;32mc:\\users\\d518_1\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0menables\u001b[0m \u001b[1;32min\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mline\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mplots\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnotebooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m   \"\"\"\n\u001b[1;32m--> 278\u001b[1;33m   dot = model_to_dot(model,\n\u001b[0m\u001b[0;32m    279\u001b[0m                      \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m                      \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_layer_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d518_1\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'IPython.core.magics.namespace'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m       \u001b[1;31m# We don't raise an exception here in order to avoid crashing notebook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d518_1\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mcheck_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# Attempt to create an image of a blank graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d518_1\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format)\u001b[0m\n\u001b[0;32m   1878\u001b[0m                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\u001b[0;32m   1879\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1880\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1881\u001b[0m                 raise Exception(\n\u001b[0;32m   1882\u001b[0m                     '\"{prog}\" not found in path.'.format(\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'os' has no attribute 'errno'"
     ]
    }
   ],
   "source": [
    "import pydot, graphviz \n",
    "tf.keras.utils.plot_model(\n",
    "    model, to_file='model.png', show_shapes=False, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4011\n",
      "4011\n",
      "1003\n",
      "1003\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print(len(x_val))\n",
    "print(len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxPr-GKLD4W3"
   },
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIEw_SmvD8of"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 0.6775 - accuracy: 0.5734 - val_loss: 0.6484 - val_accuracy: 0.6271\n",
      "Epoch 2/10000\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.5785 - accuracy: 0.6928 - val_loss: 0.6342 - val_accuracy: 0.6361\n",
      "Epoch 3/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.3954 - accuracy: 0.8330 - val_loss: 0.7248 - val_accuracy: 0.6461\n",
      "Epoch 4/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.2714 - accuracy: 0.8886 - val_loss: 0.8276 - val_accuracy: 0.6471\n",
      "Epoch 5/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.2216 - accuracy: 0.9080 - val_loss: 0.9822 - val_accuracy: 0.6381\n",
      "Epoch 6/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1797 - accuracy: 0.9175 - val_loss: 1.1304 - val_accuracy: 0.6391\n",
      "Epoch 7/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1622 - accuracy: 0.9225 - val_loss: 1.1394 - val_accuracy: 0.6321\n",
      "Epoch 8/10000\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.1544 - accuracy: 0.9282 - val_loss: 1.4822 - val_accuracy: 0.6311\n",
      "Epoch 9/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1463 - accuracy: 0.9250 - val_loss: 1.3012 - val_accuracy: 0.6421\n",
      "Epoch 10/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1318 - accuracy: 0.9332 - val_loss: 1.3278 - val_accuracy: 0.6401\n",
      "Epoch 11/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1261 - accuracy: 0.9324 - val_loss: 1.7371 - val_accuracy: 0.6351\n",
      "Epoch 12/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1226 - accuracy: 0.9359 - val_loss: 1.6382 - val_accuracy: 0.6341\n",
      "Epoch 13/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1168 - accuracy: 0.9369 - val_loss: 1.8661 - val_accuracy: 0.6191\n",
      "Epoch 14/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1208 - accuracy: 0.9344 - val_loss: 1.7364 - val_accuracy: 0.6301\n",
      "Epoch 15/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1147 - accuracy: 0.9364 - val_loss: 1.6388 - val_accuracy: 0.6341\n",
      "Epoch 16/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1119 - accuracy: 0.9367 - val_loss: 1.8920 - val_accuracy: 0.6321\n",
      "Epoch 17/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1166 - accuracy: 0.9399 - val_loss: 1.9028 - val_accuracy: 0.6251\n",
      "Epoch 18/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1160 - accuracy: 0.9397 - val_loss: 1.8699 - val_accuracy: 0.6311\n",
      "Epoch 19/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1107 - accuracy: 0.9427 - val_loss: 1.8101 - val_accuracy: 0.6291\n",
      "Epoch 20/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1104 - accuracy: 0.9399 - val_loss: 1.9157 - val_accuracy: 0.6361\n",
      "Epoch 21/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1079 - accuracy: 0.9424 - val_loss: 1.9924 - val_accuracy: 0.6231\n",
      "Epoch 22/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1057 - accuracy: 0.9429 - val_loss: 1.7821 - val_accuracy: 0.6311\n",
      "Epoch 23/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0992 - accuracy: 0.9447 - val_loss: 2.0810 - val_accuracy: 0.6211\n",
      "Epoch 24/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.9437 - val_loss: 2.2339 - val_accuracy: 0.6351\n",
      "Epoch 25/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1051 - accuracy: 0.9432 - val_loss: 2.1672 - val_accuracy: 0.6211\n",
      "Epoch 26/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1142 - accuracy: 0.9399 - val_loss: 1.8133 - val_accuracy: 0.6241\n",
      "Epoch 27/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.9424 - val_loss: 2.1001 - val_accuracy: 0.6311\n",
      "Epoch 28/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 0.9476 - val_loss: 2.1013 - val_accuracy: 0.6132\n",
      "Epoch 29/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0986 - accuracy: 0.9442 - val_loss: 2.1462 - val_accuracy: 0.6261\n",
      "Epoch 30/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0951 - accuracy: 0.9474 - val_loss: 2.2404 - val_accuracy: 0.6301\n",
      "Epoch 31/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0935 - accuracy: 0.9494 - val_loss: 2.5857 - val_accuracy: 0.6510\n",
      "Epoch 32/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0969 - accuracy: 0.9447 - val_loss: 2.2554 - val_accuracy: 0.6391\n",
      "Epoch 33/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1001 - accuracy: 0.9489 - val_loss: 2.0661 - val_accuracy: 0.6411\n",
      "Epoch 34/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.1009 - accuracy: 0.9469 - val_loss: 1.9544 - val_accuracy: 0.6381\n",
      "Epoch 35/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0957 - accuracy: 0.9514 - val_loss: 2.0204 - val_accuracy: 0.6351\n",
      "Epoch 36/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0980 - accuracy: 0.9491 - val_loss: 1.9671 - val_accuracy: 0.6351\n",
      "Epoch 37/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0910 - accuracy: 0.9496 - val_loss: 2.2930 - val_accuracy: 0.6491\n",
      "Epoch 38/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0951 - accuracy: 0.9516 - val_loss: 2.4396 - val_accuracy: 0.6381\n",
      "Epoch 39/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 0.9491 - val_loss: 2.0830 - val_accuracy: 0.6441\n",
      "Epoch 40/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0976 - accuracy: 0.9459 - val_loss: 2.1895 - val_accuracy: 0.6371\n",
      "Epoch 41/10000\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.0885 - accuracy: 0.9504 - val_loss: 2.2973 - val_accuracy: 0.6321\n",
      "Epoch 42/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0902 - accuracy: 0.9511 - val_loss: 2.5019 - val_accuracy: 0.6321\n",
      "Epoch 43/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0890 - accuracy: 0.9541 - val_loss: 2.1520 - val_accuracy: 0.6251\n",
      "Epoch 44/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.9501 - val_loss: 2.4518 - val_accuracy: 0.6331\n",
      "Epoch 45/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0869 - accuracy: 0.9541 - val_loss: 2.7124 - val_accuracy: 0.6351\n",
      "Epoch 46/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0900 - accuracy: 0.9534 - val_loss: 2.3993 - val_accuracy: 0.6301\n",
      "Epoch 47/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0893 - accuracy: 0.9516 - val_loss: 2.5057 - val_accuracy: 0.6261\n",
      "Epoch 48/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0867 - accuracy: 0.9534 - val_loss: 2.4489 - val_accuracy: 0.6271\n",
      "Epoch 49/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0854 - accuracy: 0.9516 - val_loss: 2.5279 - val_accuracy: 0.6351\n",
      "Epoch 50/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0865 - accuracy: 0.9531 - val_loss: 2.4763 - val_accuracy: 0.6301\n",
      "Epoch 51/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0863 - accuracy: 0.9529 - val_loss: 2.5707 - val_accuracy: 0.6271\n",
      "Epoch 52/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0867 - accuracy: 0.9529 - val_loss: 2.7136 - val_accuracy: 0.6321\n",
      "Epoch 53/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0848 - accuracy: 0.9514 - val_loss: 2.5515 - val_accuracy: 0.6431\n",
      "Epoch 54/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0854 - accuracy: 0.9549 - val_loss: 2.4193 - val_accuracy: 0.6261\n",
      "Epoch 55/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0836 - accuracy: 0.9539 - val_loss: 2.7105 - val_accuracy: 0.6321\n",
      "Epoch 56/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0880 - accuracy: 0.9514 - val_loss: 2.4223 - val_accuracy: 0.6371\n",
      "Epoch 57/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0854 - accuracy: 0.9524 - val_loss: 2.4559 - val_accuracy: 0.6361\n",
      "Epoch 58/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0860 - accuracy: 0.9514 - val_loss: 2.2772 - val_accuracy: 0.6321\n",
      "Epoch 59/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0853 - accuracy: 0.9519 - val_loss: 2.7177 - val_accuracy: 0.6311\n",
      "Epoch 60/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0915 - accuracy: 0.9509 - val_loss: 2.2333 - val_accuracy: 0.6341\n",
      "Epoch 61/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0879 - accuracy: 0.9526 - val_loss: 2.2933 - val_accuracy: 0.6381\n",
      "Epoch 62/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0866 - accuracy: 0.9519 - val_loss: 2.7590 - val_accuracy: 0.6361\n",
      "Epoch 63/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0853 - accuracy: 0.9536 - val_loss: 2.6808 - val_accuracy: 0.6301\n",
      "Epoch 64/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.9554 - val_loss: 2.5263 - val_accuracy: 0.6361\n",
      "Epoch 65/10000\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.0871 - accuracy: 0.9519 - val_loss: 2.6520 - val_accuracy: 0.6391\n",
      "Epoch 66/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0861 - accuracy: 0.9541 - val_loss: 2.5928 - val_accuracy: 0.6351\n",
      "Epoch 67/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0908 - accuracy: 0.9519 - val_loss: 2.2811 - val_accuracy: 0.6351\n",
      "Epoch 68/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0834 - accuracy: 0.9556 - val_loss: 2.1274 - val_accuracy: 0.6271\n",
      "Epoch 69/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0812 - accuracy: 0.9549 - val_loss: 2.4297 - val_accuracy: 0.6341\n",
      "Epoch 70/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0802 - accuracy: 0.9551 - val_loss: 2.6151 - val_accuracy: 0.6361\n",
      "Epoch 71/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0808 - accuracy: 0.9584 - val_loss: 2.6692 - val_accuracy: 0.6361\n",
      "Epoch 72/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0842 - accuracy: 0.9544 - val_loss: 2.4561 - val_accuracy: 0.6251\n",
      "Epoch 73/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0800 - accuracy: 0.9561 - val_loss: 2.9162 - val_accuracy: 0.6331\n",
      "Epoch 74/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.9556 - val_loss: 2.6763 - val_accuracy: 0.6361\n",
      "Epoch 75/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0817 - accuracy: 0.9554 - val_loss: 2.5552 - val_accuracy: 0.6311\n",
      "Epoch 76/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.9544 - val_loss: 2.5677 - val_accuracy: 0.6281\n",
      "Epoch 77/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0802 - accuracy: 0.9556 - val_loss: 2.7707 - val_accuracy: 0.6321\n",
      "Epoch 78/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0856 - accuracy: 0.9549 - val_loss: 2.3846 - val_accuracy: 0.6341\n",
      "Epoch 79/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0831 - accuracy: 0.9544 - val_loss: 2.2928 - val_accuracy: 0.6271\n",
      "Epoch 80/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0822 - accuracy: 0.9546 - val_loss: 2.4464 - val_accuracy: 0.6401\n",
      "Epoch 81/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0815 - accuracy: 0.9569 - val_loss: 2.6478 - val_accuracy: 0.6491\n",
      "Epoch 82/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0796 - accuracy: 0.9551 - val_loss: 2.7960 - val_accuracy: 0.6311\n",
      "Epoch 83/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0794 - accuracy: 0.9556 - val_loss: 2.8777 - val_accuracy: 0.6281\n",
      "Epoch 84/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0828 - accuracy: 0.9559 - val_loss: 2.7394 - val_accuracy: 0.6281\n",
      "Epoch 85/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0834 - accuracy: 0.9551 - val_loss: 2.6341 - val_accuracy: 0.6431\n",
      "Epoch 86/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0885 - accuracy: 0.9546 - val_loss: 2.1272 - val_accuracy: 0.6421\n",
      "Epoch 87/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0799 - accuracy: 0.9564 - val_loss: 2.4188 - val_accuracy: 0.6351\n",
      "Epoch 88/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0796 - accuracy: 0.9559 - val_loss: 2.5684 - val_accuracy: 0.6261\n",
      "Epoch 89/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0800 - accuracy: 0.9569 - val_loss: 2.7247 - val_accuracy: 0.6241\n",
      "Epoch 90/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0788 - accuracy: 0.9564 - val_loss: 2.6033 - val_accuracy: 0.6341\n",
      "Epoch 91/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0794 - accuracy: 0.9554 - val_loss: 3.0886 - val_accuracy: 0.6231\n",
      "Epoch 92/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.9556 - val_loss: 2.3283 - val_accuracy: 0.6391\n",
      "Epoch 93/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0825 - accuracy: 0.9554 - val_loss: 2.4049 - val_accuracy: 0.6321\n",
      "Epoch 94/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0786 - accuracy: 0.9556 - val_loss: 2.6064 - val_accuracy: 0.6321\n",
      "Epoch 95/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0792 - accuracy: 0.9564 - val_loss: 2.7201 - val_accuracy: 0.6231\n",
      "Epoch 96/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0789 - accuracy: 0.9576 - val_loss: 2.7018 - val_accuracy: 0.6251\n",
      "Epoch 97/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.9594 - val_loss: 2.8677 - val_accuracy: 0.6371\n",
      "Epoch 98/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0766 - accuracy: 0.9586 - val_loss: 2.9439 - val_accuracy: 0.6311\n",
      "Epoch 99/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0797 - accuracy: 0.9566 - val_loss: 2.8161 - val_accuracy: 0.6231\n",
      "Epoch 100/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.9564 - val_loss: 2.9919 - val_accuracy: 0.6421\n",
      "Epoch 101/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0776 - accuracy: 0.9574 - val_loss: 2.9284 - val_accuracy: 0.6401\n",
      "Epoch 102/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0778 - accuracy: 0.9556 - val_loss: 3.2012 - val_accuracy: 0.6411\n",
      "Epoch 103/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0786 - accuracy: 0.9559 - val_loss: 2.9967 - val_accuracy: 0.6391\n",
      "Epoch 104/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0782 - accuracy: 0.9584 - val_loss: 3.1033 - val_accuracy: 0.6241\n",
      "Epoch 105/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0809 - accuracy: 0.9556 - val_loss: 2.9008 - val_accuracy: 0.6251\n",
      "Epoch 106/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9549 - val_loss: 3.0398 - val_accuracy: 0.6321\n",
      "Epoch 107/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0869 - accuracy: 0.9519 - val_loss: 2.3308 - val_accuracy: 0.6271\n",
      "Epoch 108/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0820 - accuracy: 0.9559 - val_loss: 2.4021 - val_accuracy: 0.6181\n",
      "Epoch 109/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0854 - accuracy: 0.9564 - val_loss: 2.2097 - val_accuracy: 0.6301\n",
      "Epoch 110/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.9556 - val_loss: 2.1196 - val_accuracy: 0.6241\n",
      "Epoch 111/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0772 - accuracy: 0.9579 - val_loss: 2.6922 - val_accuracy: 0.6271\n",
      "Epoch 112/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.9544 - val_loss: 2.5815 - val_accuracy: 0.6211\n",
      "Epoch 113/10000\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0789 - accuracy: 0.9561 - val_loss: 3.0337 - val_accuracy: 0.6221\n",
      "Epoch 114/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.9541 - val_loss: 2.6087 - val_accuracy: 0.6162\n",
      "Epoch 115/10000\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0800 - accuracy: 0.9564 - val_loss: 2.8030 - val_accuracy: 0.6162\n",
      "Epoch 116/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0773 - accuracy: 0.9576 - val_loss: 3.1359 - val_accuracy: 0.6211\n",
      "Epoch 117/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0824 - accuracy: 0.9569 - val_loss: 2.6467 - val_accuracy: 0.6261\n",
      "Epoch 118/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 0.9584 - val_loss: 2.7590 - val_accuracy: 0.6211\n",
      "Epoch 119/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0784 - accuracy: 0.9574 - val_loss: 2.9925 - val_accuracy: 0.6231\n",
      "Epoch 120/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0771 - accuracy: 0.9556 - val_loss: 2.9617 - val_accuracy: 0.6271\n",
      "Epoch 121/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0764 - accuracy: 0.9576 - val_loss: 3.1219 - val_accuracy: 0.6271\n",
      "Epoch 122/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9579 - val_loss: 3.1627 - val_accuracy: 0.6201\n",
      "Epoch 123/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 0.9599 - val_loss: 3.2987 - val_accuracy: 0.6291\n",
      "Epoch 124/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0757 - accuracy: 0.9574 - val_loss: 3.4954 - val_accuracy: 0.6271\n",
      "Epoch 125/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9571 - val_loss: 3.4713 - val_accuracy: 0.6291\n",
      "Epoch 126/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0764 - accuracy: 0.9581 - val_loss: 3.3909 - val_accuracy: 0.6271\n",
      "Epoch 127/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9564 - val_loss: 3.5349 - val_accuracy: 0.6241\n",
      "Epoch 128/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0746 - accuracy: 0.9594 - val_loss: 3.4570 - val_accuracy: 0.6231\n",
      "Epoch 129/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0783 - accuracy: 0.9576 - val_loss: 3.5451 - val_accuracy: 0.6331\n",
      "Epoch 130/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0761 - accuracy: 0.9561 - val_loss: 3.4565 - val_accuracy: 0.6271\n",
      "Epoch 131/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0762 - accuracy: 0.9581 - val_loss: 3.4393 - val_accuracy: 0.6221\n",
      "Epoch 132/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0755 - accuracy: 0.9561 - val_loss: 3.5843 - val_accuracy: 0.6281\n",
      "Epoch 133/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 0.9526 - val_loss: 1.8288 - val_accuracy: 0.6201\n",
      "Epoch 134/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0820 - accuracy: 0.9561 - val_loss: 2.5439 - val_accuracy: 0.6092\n",
      "Epoch 135/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.9569 - val_loss: 2.7604 - val_accuracy: 0.6132\n",
      "Epoch 136/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9586 - val_loss: 2.7297 - val_accuracy: 0.6181\n",
      "Epoch 137/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0773 - accuracy: 0.9561 - val_loss: 3.0381 - val_accuracy: 0.6132\n",
      "Epoch 138/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9561 - val_loss: 2.6240 - val_accuracy: 0.6171\n",
      "Epoch 139/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0773 - accuracy: 0.9564 - val_loss: 2.8860 - val_accuracy: 0.6231\n",
      "Epoch 140/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0773 - accuracy: 0.9569 - val_loss: 2.7032 - val_accuracy: 0.6221\n",
      "Epoch 141/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0750 - accuracy: 0.9576 - val_loss: 3.1395 - val_accuracy: 0.6341\n",
      "Epoch 142/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0776 - accuracy: 0.9564 - val_loss: 2.9891 - val_accuracy: 0.6241\n",
      "Epoch 143/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 0.9574 - val_loss: 3.0416 - val_accuracy: 0.6211\n",
      "Epoch 144/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0754 - accuracy: 0.9589 - val_loss: 3.1584 - val_accuracy: 0.6281\n",
      "Epoch 145/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9566 - val_loss: 3.2433 - val_accuracy: 0.6261\n",
      "Epoch 146/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0758 - accuracy: 0.9576 - val_loss: 3.3159 - val_accuracy: 0.6231\n",
      "Epoch 147/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0751 - accuracy: 0.9589 - val_loss: 3.2990 - val_accuracy: 0.6201\n",
      "Epoch 148/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0759 - accuracy: 0.9584 - val_loss: 3.0624 - val_accuracy: 0.6241\n",
      "Epoch 149/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.9581 - val_loss: 3.0720 - val_accuracy: 0.6072\n",
      "Epoch 150/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0804 - accuracy: 0.9564 - val_loss: 2.6830 - val_accuracy: 0.6211\n",
      "Epoch 151/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0838 - accuracy: 0.9549 - val_loss: 2.2270 - val_accuracy: 0.6181\n",
      "Epoch 152/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.9569 - val_loss: 2.6036 - val_accuracy: 0.6231\n",
      "Epoch 153/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0761 - accuracy: 0.9584 - val_loss: 2.7950 - val_accuracy: 0.6311\n",
      "Epoch 154/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0748 - accuracy: 0.9584 - val_loss: 3.0896 - val_accuracy: 0.6271\n",
      "Epoch 155/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0761 - accuracy: 0.9574 - val_loss: 3.1668 - val_accuracy: 0.6191\n",
      "Epoch 156/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0748 - accuracy: 0.9569 - val_loss: 3.3175 - val_accuracy: 0.6162\n",
      "Epoch 157/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 0.9586 - val_loss: 3.1316 - val_accuracy: 0.6211\n",
      "Epoch 158/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0760 - accuracy: 0.9566 - val_loss: 3.1019 - val_accuracy: 0.6102\n",
      "Epoch 159/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0786 - accuracy: 0.9556 - val_loss: 3.1314 - val_accuracy: 0.6122\n",
      "Epoch 160/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0754 - accuracy: 0.9584 - val_loss: 3.0959 - val_accuracy: 0.6261\n",
      "Epoch 161/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0802 - accuracy: 0.9584 - val_loss: 2.7631 - val_accuracy: 0.6261\n",
      "Epoch 162/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 0.9576 - val_loss: 3.0158 - val_accuracy: 0.6291\n",
      "Epoch 163/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0751 - accuracy: 0.9579 - val_loss: 3.1717 - val_accuracy: 0.6241\n",
      "Epoch 164/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 0.9571 - val_loss: 3.2432 - val_accuracy: 0.6261\n",
      "Epoch 165/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0776 - accuracy: 0.9584 - val_loss: 3.1780 - val_accuracy: 0.6112\n",
      "Epoch 166/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0760 - accuracy: 0.9571 - val_loss: 3.1840 - val_accuracy: 0.6221\n",
      "Epoch 167/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0784 - accuracy: 0.9566 - val_loss: 2.6080 - val_accuracy: 0.6251\n",
      "Epoch 168/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0754 - accuracy: 0.9594 - val_loss: 3.2453 - val_accuracy: 0.6251\n",
      "Epoch 169/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0755 - accuracy: 0.9579 - val_loss: 2.9851 - val_accuracy: 0.6201\n",
      "Epoch 170/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0760 - accuracy: 0.9589 - val_loss: 3.2468 - val_accuracy: 0.6171\n",
      "Epoch 171/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0781 - accuracy: 0.9579 - val_loss: 2.8724 - val_accuracy: 0.6152\n",
      "Epoch 172/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0750 - accuracy: 0.9571 - val_loss: 3.4991 - val_accuracy: 0.6221\n",
      "Epoch 173/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0753 - accuracy: 0.9579 - val_loss: 3.2944 - val_accuracy: 0.6271\n",
      "Epoch 174/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0746 - accuracy: 0.9609 - val_loss: 3.6424 - val_accuracy: 0.6281\n",
      "Epoch 175/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0745 - accuracy: 0.9599 - val_loss: 3.6819 - val_accuracy: 0.6181\n",
      "Epoch 176/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 0.9579 - val_loss: 3.7088 - val_accuracy: 0.6152\n",
      "Epoch 177/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0746 - accuracy: 0.9591 - val_loss: 3.8390 - val_accuracy: 0.6281\n",
      "Epoch 178/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0764 - accuracy: 0.9574 - val_loss: 3.3304 - val_accuracy: 0.6281\n",
      "Epoch 179/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0748 - accuracy: 0.9584 - val_loss: 3.5084 - val_accuracy: 0.6261\n",
      "Epoch 180/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0753 - accuracy: 0.9579 - val_loss: 3.3655 - val_accuracy: 0.6211\n",
      "Epoch 181/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0741 - accuracy: 0.9586 - val_loss: 3.7745 - val_accuracy: 0.6291\n",
      "Epoch 182/10000\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0742 - accuracy: 0.9594 - val_loss: 3.7953 - val_accuracy: 0.6251\n",
      "Epoch 183/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0736 - accuracy: 0.9601 - val_loss: 3.6941 - val_accuracy: 0.6221\n",
      "Epoch 184/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.9581 - val_loss: 3.9234 - val_accuracy: 0.6251\n",
      "Epoch 185/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9581 - val_loss: 3.9139 - val_accuracy: 0.6211\n",
      "Epoch 186/10000\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.0734 - accuracy: 0.9604 - val_loss: 3.9708 - val_accuracy: 0.6251\n",
      "Epoch 187/10000\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.0865 - accuracy: 0.9569 - val_loss: 2.9165 - val_accuracy: 0.6291\n",
      "Epoch 188/10000\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0869 - accuracy: 0.9561 - val_loss: 2.3558 - val_accuracy: 0.6191\n",
      "Epoch 189/10000\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0765 - accuracy: 0.9566 - val_loss: 3.0187 - val_accuracy: 0.6211\n",
      "Epoch 190/10000\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0810 - accuracy: 0.9554 - val_loss: 2.3390 - val_accuracy: 0.6281\n",
      "Epoch 191/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0742 - accuracy: 0.9599 - val_loss: 2.7756 - val_accuracy: 0.6331\n",
      "Epoch 192/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0753 - accuracy: 0.9594 - val_loss: 2.7959 - val_accuracy: 0.6201\n",
      "Epoch 193/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.9584 - val_loss: 3.0081 - val_accuracy: 0.6221\n",
      "Epoch 194/10000\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0745 - accuracy: 0.9591 - val_loss: 3.0575 - val_accuracy: 0.6191\n",
      "Epoch 195/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0745 - accuracy: 0.9564 - val_loss: 3.1490 - val_accuracy: 0.6162\n",
      "Epoch 196/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0745 - accuracy: 0.9581 - val_loss: 3.2176 - val_accuracy: 0.6132\n",
      "Epoch 197/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9594 - val_loss: 3.4244 - val_accuracy: 0.6181\n",
      "Epoch 198/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.9574 - val_loss: 3.4531 - val_accuracy: 0.6211\n",
      "Epoch 199/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0730 - accuracy: 0.9596 - val_loss: 3.2196 - val_accuracy: 0.6221\n",
      "Epoch 200/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9579 - val_loss: 3.3395 - val_accuracy: 0.6221\n",
      "Epoch 201/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0828 - accuracy: 0.9561 - val_loss: 2.9374 - val_accuracy: 0.6142\n",
      "Epoch 202/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0780 - accuracy: 0.9589 - val_loss: 2.6186 - val_accuracy: 0.6102\n",
      "Epoch 203/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.9584 - val_loss: 2.9500 - val_accuracy: 0.6181\n",
      "Epoch 204/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0742 - accuracy: 0.9586 - val_loss: 3.0682 - val_accuracy: 0.6132\n",
      "Epoch 205/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9594 - val_loss: 2.8957 - val_accuracy: 0.6162\n",
      "Epoch 206/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9591 - val_loss: 3.0827 - val_accuracy: 0.6122\n",
      "Epoch 207/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.9559 - val_loss: 3.1499 - val_accuracy: 0.6152\n",
      "Epoch 208/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9596 - val_loss: 3.2670 - val_accuracy: 0.6102\n",
      "Epoch 209/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0735 - accuracy: 0.9571 - val_loss: 3.3331 - val_accuracy: 0.6092\n",
      "Epoch 210/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0738 - accuracy: 0.9579 - val_loss: 3.4142 - val_accuracy: 0.6142\n",
      "Epoch 211/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.9584 - val_loss: 3.3093 - val_accuracy: 0.6092\n",
      "Epoch 212/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0740 - accuracy: 0.9586 - val_loss: 3.4994 - val_accuracy: 0.6122\n",
      "Epoch 213/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0736 - accuracy: 0.9604 - val_loss: 3.5101 - val_accuracy: 0.6142\n",
      "Epoch 214/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0727 - accuracy: 0.9581 - val_loss: 3.5213 - val_accuracy: 0.6241\n",
      "Epoch 215/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0735 - accuracy: 0.9579 - val_loss: 3.6020 - val_accuracy: 0.6251\n",
      "Epoch 216/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0766 - accuracy: 0.9571 - val_loss: 3.0393 - val_accuracy: 0.6062\n",
      "Epoch 217/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0770 - accuracy: 0.9591 - val_loss: 3.1190 - val_accuracy: 0.6181\n",
      "Epoch 218/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0822 - accuracy: 0.9559 - val_loss: 3.0245 - val_accuracy: 0.6122\n",
      "Epoch 219/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0810 - accuracy: 0.9559 - val_loss: 2.3470 - val_accuracy: 0.6241\n",
      "Epoch 220/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.9594 - val_loss: 2.6781 - val_accuracy: 0.6181\n",
      "Epoch 221/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0733 - accuracy: 0.9594 - val_loss: 2.9170 - val_accuracy: 0.6241\n",
      "Epoch 222/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0747 - accuracy: 0.9576 - val_loss: 2.8199 - val_accuracy: 0.6162\n",
      "Epoch 223/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0741 - accuracy: 0.9591 - val_loss: 3.0956 - val_accuracy: 0.6201\n",
      "Epoch 224/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0751 - accuracy: 0.9581 - val_loss: 2.9974 - val_accuracy: 0.6241\n",
      "Epoch 225/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0738 - accuracy: 0.9594 - val_loss: 3.0851 - val_accuracy: 0.6142\n",
      "Epoch 226/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0745 - accuracy: 0.9581 - val_loss: 3.1850 - val_accuracy: 0.6152\n",
      "Epoch 227/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0730 - accuracy: 0.9591 - val_loss: 3.2555 - val_accuracy: 0.6181\n",
      "Epoch 228/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9591 - val_loss: 3.3126 - val_accuracy: 0.6201\n",
      "Epoch 229/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0741 - accuracy: 0.9589 - val_loss: 3.4198 - val_accuracy: 0.6181\n",
      "Epoch 230/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.9574 - val_loss: 3.5782 - val_accuracy: 0.6231\n",
      "Epoch 231/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.9586 - val_loss: 3.1753 - val_accuracy: 0.6162\n",
      "Epoch 232/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9604 - val_loss: 3.3359 - val_accuracy: 0.6201\n",
      "Epoch 233/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.9579 - val_loss: 3.3741 - val_accuracy: 0.6152\n",
      "Epoch 234/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0742 - accuracy: 0.9576 - val_loss: 3.4513 - val_accuracy: 0.6241\n",
      "Epoch 235/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0733 - accuracy: 0.9594 - val_loss: 3.5914 - val_accuracy: 0.6152\n",
      "Epoch 236/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0730 - accuracy: 0.9581 - val_loss: 3.6014 - val_accuracy: 0.6191\n",
      "Epoch 237/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.9584 - val_loss: 3.7131 - val_accuracy: 0.6201\n",
      "Epoch 238/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0745 - accuracy: 0.9574 - val_loss: 3.7009 - val_accuracy: 0.6171\n",
      "Epoch 239/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0735 - accuracy: 0.9576 - val_loss: 3.7000 - val_accuracy: 0.6231\n",
      "Epoch 240/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0735 - accuracy: 0.9586 - val_loss: 3.6798 - val_accuracy: 0.6152\n",
      "Epoch 241/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0742 - accuracy: 0.9566 - val_loss: 3.7545 - val_accuracy: 0.6181\n",
      "Epoch 242/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.9594 - val_loss: 3.3460 - val_accuracy: 0.6231\n",
      "Epoch 243/10000\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0801 - accuracy: 0.9586 - val_loss: 2.8146 - val_accuracy: 0.6341\n",
      "Epoch 244/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0826 - accuracy: 0.9579 - val_loss: 2.2716 - val_accuracy: 0.6271\n",
      "Epoch 245/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0783 - accuracy: 0.9559 - val_loss: 2.3592 - val_accuracy: 0.6261\n",
      "Epoch 246/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.9584 - val_loss: 2.6530 - val_accuracy: 0.6231\n",
      "Epoch 247/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.9571 - val_loss: 2.8876 - val_accuracy: 0.6261\n",
      "Epoch 248/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9591 - val_loss: 3.0806 - val_accuracy: 0.6251\n",
      "Epoch 249/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9576 - val_loss: 3.0325 - val_accuracy: 0.6281\n",
      "Epoch 250/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9566 - val_loss: 2.9979 - val_accuracy: 0.6281\n",
      "Epoch 251/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0740 - accuracy: 0.9596 - val_loss: 3.0570 - val_accuracy: 0.6291\n",
      "Epoch 252/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0740 - accuracy: 0.9584 - val_loss: 3.0905 - val_accuracy: 0.6291\n",
      "Epoch 253/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0738 - accuracy: 0.9579 - val_loss: 3.2152 - val_accuracy: 0.6231\n",
      "Epoch 254/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9591 - val_loss: 3.3253 - val_accuracy: 0.6291\n",
      "Epoch 255/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0730 - accuracy: 0.9596 - val_loss: 3.3632 - val_accuracy: 0.6221\n",
      "Epoch 256/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.9591 - val_loss: 3.2514 - val_accuracy: 0.6251\n",
      "Epoch 257/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0742 - accuracy: 0.9574 - val_loss: 3.4102 - val_accuracy: 0.6291\n",
      "Epoch 258/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0738 - accuracy: 0.9601 - val_loss: 3.4630 - val_accuracy: 0.6291\n",
      "Epoch 259/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9581 - val_loss: 3.3207 - val_accuracy: 0.6251\n",
      "Epoch 260/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9581 - val_loss: 3.4999 - val_accuracy: 0.6241\n",
      "Epoch 261/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9584 - val_loss: 3.5798 - val_accuracy: 0.6201\n",
      "Epoch 262/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9576 - val_loss: 3.5124 - val_accuracy: 0.6181\n",
      "Epoch 263/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0740 - accuracy: 0.9589 - val_loss: 3.5423 - val_accuracy: 0.6181\n",
      "Epoch 264/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0738 - accuracy: 0.9594 - val_loss: 3.6989 - val_accuracy: 0.6231\n",
      "Epoch 265/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9591 - val_loss: 3.8147 - val_accuracy: 0.6231\n",
      "Epoch 266/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0736 - accuracy: 0.9584 - val_loss: 3.7149 - val_accuracy: 0.6241\n",
      "Epoch 267/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0735 - accuracy: 0.9589 - val_loss: 3.8383 - val_accuracy: 0.6291\n",
      "Epoch 268/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0736 - accuracy: 0.9591 - val_loss: 3.9310 - val_accuracy: 0.6241\n",
      "Epoch 269/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9566 - val_loss: 3.7880 - val_accuracy: 0.6251\n",
      "Epoch 270/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0738 - accuracy: 0.9591 - val_loss: 3.6394 - val_accuracy: 0.6231\n",
      "Epoch 271/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.9601 - val_loss: 3.7219 - val_accuracy: 0.6251\n",
      "Epoch 272/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0726 - accuracy: 0.9596 - val_loss: 3.9304 - val_accuracy: 0.6281\n",
      "Epoch 273/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0767 - accuracy: 0.9576 - val_loss: 3.3562 - val_accuracy: 0.6201\n",
      "Epoch 274/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0741 - accuracy: 0.9584 - val_loss: 3.4778 - val_accuracy: 0.6241\n",
      "Epoch 275/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0732 - accuracy: 0.9596 - val_loss: 3.5600 - val_accuracy: 0.6281\n",
      "Epoch 276/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9584 - val_loss: 3.7938 - val_accuracy: 0.6281\n",
      "Epoch 277/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0820 - accuracy: 0.9576 - val_loss: 2.6797 - val_accuracy: 0.6231\n",
      "Epoch 278/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0823 - accuracy: 0.9594 - val_loss: 2.4953 - val_accuracy: 0.6162\n",
      "Epoch 279/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0800 - accuracy: 0.9589 - val_loss: 2.2675 - val_accuracy: 0.6261\n",
      "Epoch 280/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0741 - accuracy: 0.9619 - val_loss: 2.3207 - val_accuracy: 0.6281\n",
      "Epoch 281/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.9576 - val_loss: 2.6825 - val_accuracy: 0.6251\n",
      "Epoch 282/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9579 - val_loss: 2.5387 - val_accuracy: 0.6132\n",
      "Epoch 283/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0764 - accuracy: 0.9586 - val_loss: 2.6968 - val_accuracy: 0.6281\n",
      "Epoch 284/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9581 - val_loss: 2.5534 - val_accuracy: 0.6201\n",
      "Epoch 285/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9581 - val_loss: 2.7036 - val_accuracy: 0.6211\n",
      "Epoch 286/10000\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9591 - val_loss: 2.7557 - val_accuracy: 0.6181\n",
      "Epoch 287/10000\n",
      "118/126 [===========================>..] - ETA: 0s - loss: 0.0724 - accuracy: 0.95"
     ]
    }
   ],
   "source": [
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=10000, validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vioV-T3mEKLj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 2.3005 - accuracy: 0.6251\n",
      "[2.300530195236206, 0.6251246333122253]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_vloss = history.history['val_accuracy']\n",
    "y_loss = history.history['loss']\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Accuracy\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRWiJ+FCOJxiT4b0RQvU6t",
   "name": "chatting.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
